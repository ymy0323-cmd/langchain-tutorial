# -*- coding: utf-8 -*-
# Streamlit ë° ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_teddynote import logging

# í™˜ê²½ ì„¤ì •
from dotenv import load_dotenv

# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
load_dotenv(override=True)

# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com
logging.langsmith("LangGraph-Tutorial")

# Streamlit ì•± ì œëª© ì„¤ì •
st.title("ğŸ’¬ AI ì±—ë´‡ chatbot")

# Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì•± ì¬ì‹¤í–‰ ì‹œì—ë„ ëŒ€í™” ê¸°ë¡ ìœ ì§€)
if "messages" not in st.session_state:
    # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    st.session_state["messages"] = []

# ì‚¬ì´ë“œë°” UI êµ¬ì„±
with st.sidebar:
    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
    clear_btn = st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”")

    # LLM ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
    selected_model = st.selectbox(
        "âœ… LLM ëª¨ë¸ ì„ íƒ",
        ["gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"],
        index=0,
        help="ì‚¬ìš©í•  ì–¸ì–´ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.",
    )

    # Temperature ì„¤ì • (ëª¨ë¸ ì°½ì˜ì„± ì¡°ì ˆ)
    temperature = st.slider(
        "ğŸŒ¡ï¸ Temperature (ì°½ì˜ì„±)",
        min_value=0.0,
        max_value=2.0,
        value=0.7,
        step=0.1,
        help="0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •í™•í•˜ê³  ì¼ê´€ëœ ë‹µë³€, 2ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ ë‹µë³€",
    )

    # ë‹µë³€ ê¸¸ì´ ì¡°ì ˆ ì„ íƒë°•ìŠ¤
    response_length = st.selectbox(
        "ğŸ“ ë‹µë³€ ê¸¸ì´ ì„¤ì •",
        ["ê°„ë‹¨", "ë³´í†µ", "ìì„¸í•¨", "ë§¤ìš° ìì„¸í•¨"],
        index=1,
        help="AI ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.",
    )

    st.divider()

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    st.subheader("âš™ï¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
    system_prompt = st.text_area(
        "AIì˜ ì—­í• ê³¼ ì„±ê²©ì„ ì •ì˜í•˜ì„¸ìš”",
        value="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ê³  ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ë©°, í•„ìš”ì‹œ ì¶”ê°€ ì •ë³´ë‚˜ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.",
        height=150,
        help="AIì˜ ì—­í• , ì„±ê²©, ë‹µë³€ ìŠ¤íƒ€ì¼ ë“±ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    )


# ì´ì „ ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
def print_messages():
    """ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ ìˆœì„œëŒ€ë¡œ í™”ë©´ì— í‘œì‹œ"""
    if st.session_state["messages"]:
        for chat_message in st.session_state["messages"]:
            st.chat_message(chat_message.role).write(chat_message.content)
    else:
        st.info("ğŸ’­ ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.")


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
def add_message(role, message):
    """ìƒˆë¡œìš´ ëŒ€í™” ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥"""
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# ë‹µë³€ ê¸¸ì´ì— ë”°ë¥¸ ì§€ì‹œì‚¬í•­ ìƒì„±
def get_length_instruction(length):
    """ì„ íƒëœ ë‹µë³€ ê¸¸ì´ì— ë”°ë¥¸ ì§€ì‹œì‚¬í•­ ë°˜í™˜"""
    length_map = {
        "ê°„ë‹¨": "ê°„ë‹¨í•˜ê³  í•µì‹¬ì ì¸ ë‹µë³€ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.",
        "ë³´í†µ": "ì ì ˆí•œ ê¸¸ì´ë¡œ ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. (2-3ë¬¸ë‹¨ ì •ë„)",
        "ìì„¸í•¨": "ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ì˜ˆì‹œë‚˜ ì¶”ê°€ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”.",
        "ë§¤ìš° ìì„¸í•¨": "ë§¤ìš° ìƒì„¸í•˜ê³  ì‹¬ì¸µì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ë‹¤ì–‘í•œ ê´€ì ê³¼ ì˜ˆì‹œ, ê´€ë ¨ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”.",
    }
    return length_map.get(length, "ì ì ˆí•œ ê¸¸ì´ë¡œ ë‹µë³€í•˜ì„¸ìš”.")


# AI ë‹µë³€ ìƒì„± í•¨ìˆ˜
def generate_answer(
    user_input, system_prompt, model_name, temperature, response_length
):
    """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ AI ë‹µë³€ ìƒì„±"""
    try:
        # OpenAI ëª¨ë¸ ì´ˆê¸°í™”
        llm = ChatOpenAI(
            model=model_name,
            temperature=temperature,
        )

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë‹µë³€ ê¸¸ì´ ì§€ì‹œì‚¬í•­ ì¶”ê°€
        length_instruction = get_length_instruction(response_length)
        enhanced_system_prompt = f"{system_prompt}\n\në‹µë³€ ìŠ¤íƒ€ì¼: {length_instruction}"

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            SystemMessage(content=enhanced_system_prompt),
            HumanMessage(content=user_input),
        ]

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        response = llm.stream(messages)
        return response

    except Exception as e:
        raise RuntimeError(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ í´ë¦­ ì‹œ
if clear_btn:
    st.session_state["messages"] = []
    st.rerun()

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ì°½
user_input = st.chat_input("ğŸ’¬ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

# ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±
if user_input:
    try:
        # AI ë‹µë³€ ìƒì„±
        response = generate_answer(
            user_input=user_input,
            system_prompt=system_prompt,
            model_name=selected_model,
            temperature=temperature,
            response_length=response_length,
        )

        # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
        st.chat_message("user").write(user_input)

        # AI ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ í‘œì‹œ
        with st.chat_message("assistant"):
            ai_answer = st.write_stream(response)

        # ëŒ€í™” ê¸°ë¡ì„ ì„¸ì…˜ì— ì €ì¥
        add_message("user", user_input)
        add_message("assistant", ai_answer)

    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.info("ğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜, ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•´ ë³´ì„¸ìš”.")

# ì‚¬ì´ë“œë°” í•˜ë‹¨ì— í˜„ì¬ ì„¤ì • ì •ë³´ í‘œì‹œ
with st.sidebar:
    st.divider()
    st.markdown("### ğŸ“Š í˜„ì¬ ì„¤ì •")
    st.caption(f"**ëª¨ë¸:** {selected_model}")
    st.caption(f"**Temperature:** {temperature}")
    st.caption(f"**ë‹µë³€ ê¸¸ì´:** {response_length}")
